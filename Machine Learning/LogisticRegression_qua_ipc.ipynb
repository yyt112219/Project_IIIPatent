{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"yarn\")\\\n",
    ".config('spark.executor.cores', '2')\\\n",
    ".config('spark.executor.memory', '7G')\\\n",
    ".config('spark.driver.memory','4G')\\\n",
    ".config('spark.executor.instances','8')\\\n",
    ".appName('bdse62')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract=spark.read.csv('hdfs:///bdse71/ABS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = spark.read.parquet('hdfs:///data/jso_hierarchy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+-------+-------+\n",
      "|Application_Number|mono_ipc|tri_ipc|qua_ipc|\n",
      "+------------------+--------+-------+-------+\n",
      "| PCT/CA2018/000243|       B|    A61|   B25J|\n",
      "| PCT/CA2019/050200|       G|    G02|   G02C|\n",
      "| PCT/CL2016/050019|       G|    G06|   G06Q|\n",
      "| PCT/CN2012/070253|       A|    A43|   A43B|\n",
      "| PCT/DE2014/100160|       F|    F41|   F41H|\n",
      "| PCT/DE2016/100332|       C|    C10|   C10B|\n",
      "| PCT/DK2005/000435|       A|    A61|   A61K|\n",
      "| PCT/EP1999/004038|       E|    E21|   E21B|\n",
      "| PCT/EP1999/008598|       A|    A61|   A61K|\n",
      "| PCT/EP2000/004724|       A|    A23|   A23G|\n",
      "| PCT/EP2000/011562|       C|    C11|   C11D|\n",
      "| PCT/EP2001/002279|       C|    A01|   A01N|\n",
      "| PCT/EP2001/003614|       B|    F16|   F16B|\n",
      "| PCT/EP2001/003789|       A|    A22|   A22B|\n",
      "| PCT/EP2001/010090|       C|    C12|   C12N|\n",
      "| PCT/EP2001/013071|       C|    C07|   C08F|\n",
      "| PCT/EP2001/014943|       B|    B60|   B60T|\n",
      "| PCT/EP2003/007473|       C|    A01|   A01N|\n",
      "| PCT/EP2003/011110|       A|    A61|   A61K|\n",
      "| PCT/EP2005/006018|       C|    A01|   A01N|\n",
      "+------------------+--------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testDF = spark.read.json('hdfs:///data/df_mono_ipc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|              _c0|                 _c1|\n",
      "+-----------------+--------------------+\n",
      "|PCT/IB2015/050868|For the productio...|\n",
      "|PCT/EP2013/003311|A composition com...|\n",
      "|PCT/AU2010/000148|An agricultural a...|\n",
      "|PCT/CN2013/072341|A detection metho...|\n",
      "|PCT/EP2013/058979|The invention rel...|\n",
      "|PCT/EP2016/063487|The present inven...|\n",
      "|PCT/GB2009/001774|A compound having...|\n",
      "|PCT/EP2013/077705|A compound with t...|\n",
      "|PCT/EP2009/006204|The invention rel...|\n",
      "|PCT/KR2017/010450|The present inven...|\n",
      "|PCT/EP2010/000270|The present inven...|\n",
      "|PCT/EP2013/077695|A compound of for...|\n",
      "|PCT/KR2012/004595|The present funct...|\n",
      "|PCT/JP2015/004803|A zoom lens accor...|\n",
      "|PCT/AT2017/000070|The invention rel...|\n",
      "|PCT/KR2015/006148|The present inven...|\n",
      "|PCT/IB2014/001029|A conjugate of fo...|\n",
      "|PCT/GB2012/052526|A method for prom...|\n",
      "|PCT/EP2001/009832|Liquid crystal mi...|\n",
      "|PCT/KR2000/000814|The present inven...|\n",
      "+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abstract=abstract.dropna()\n",
    "abstract.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "abstractJoin two tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression = testDF['Application_Number']==abstract[\"_c0\"]\n",
    "joinType = \"inner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Application_Number,StringType,true),StructField(mono_ipc,StringType,true),StructField(tri_ipc,StringType,true),StructField(qua_ipc,StringType,true)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+-------+-------+--------------------+\n",
      "|Application_Number|mono_ipc|tri_ipc|qua_ipc|                 _c1|\n",
      "+------------------+--------+-------+-------+--------------------+\n",
      "| PCT/CN2014/000929|       E|    E21|   E21C|A harrow excavato...|\n",
      "| PCT/EP2000/011063|       E|    E05|   E05B|The invention rel...|\n",
      "| PCT/DE2000/002722|       E|    E05|   G06F|The invention rel...|\n",
      "| PCT/EP2001/011433|       E|    E01|   E02D|The invention rel...|\n",
      "| PCT/IB2016/055812|       E|    E21|   E02D|A percussion devi...|\n",
      "| PCT/KR2015/007369|       E|    E01|   E01C|The present inven...|\n",
      "| PCT/US2001/006951|       E|    E21|   E21B|A petroleum well ...|\n",
      "| PCT/EP2001/003208|       E|    E05|   E05B|The invention rel...|\n",
      "| PCT/EP2012/069088|       E|    E21|   E21B|The present inven...|\n",
      "| PCT/CN2015/071782|       E|    E21|   E21C|A reciprocating i...|\n",
      "| PCT/CA2001/000121|       E|    E05|   E05B|A latch assembly ...|\n",
      "| PCT/CN2014/000009|       E|    E21|   E21C|An easily removab...|\n",
      "| PCT/AU2016/050241|       E|    E21|   E21B|Orientation data ...|\n",
      "| PCT/US2001/009607|       E|    E21|   E21B|A flow completion...|\n",
      "| PCT/CA2000/001056|       E|    E05|   E05B|A latch assembly ...|\n",
      "| PCT/SE2002/000043|       E|    E04|   E04F|A floorboard and ...|\n",
      "| PCT/NO2010/000081|       E|    E21|   E21B|Method and system...|\n",
      "| PCT/ES2002/000118|       E|    E01|   E01F|The invention rel...|\n",
      "| PCT/IB2013/000890|       E|    E21|   E21B|The present inven...|\n",
      "| PCT/EP2009/067481|       E|    E04|   E04B|The present inven...|\n",
      "+------------------+--------+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF=testDF.join(abstract,joinExpression,joinType).drop('_c0')\n",
    "joinedDF.filter(joinedDF.mono_ipc=='E').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+--------------------+\n",
      "|Application_Number|qua_ipc|                 _c1|\n",
      "+------------------+-------+--------------------+\n",
      "| PCT/CN2014/000929|   E21C|A harrow excavato...|\n",
      "| PCT/EP2000/011063|   E05B|The invention rel...|\n",
      "| PCT/DE2000/002722|   G06F|The invention rel...|\n",
      "| PCT/EP2001/011433|   E02D|The invention rel...|\n",
      "| PCT/IB2016/055812|   E02D|A percussion devi...|\n",
      "| PCT/KR2015/007369|   E01C|The present inven...|\n",
      "| PCT/US2001/006951|   E21B|A petroleum well ...|\n",
      "| PCT/EP2001/003208|   E05B|The invention rel...|\n",
      "| PCT/EP2012/069088|   E21B|The present inven...|\n",
      "| PCT/CN2015/071782|   E21C|A reciprocating i...|\n",
      "| PCT/CA2001/000121|   E05B|A latch assembly ...|\n",
      "| PCT/CN2014/000009|   E21C|An easily removab...|\n",
      "| PCT/AU2016/050241|   E21B|Orientation data ...|\n",
      "| PCT/US2001/009607|   E21B|A flow completion...|\n",
      "| PCT/CA2000/001056|   E05B|A latch assembly ...|\n",
      "| PCT/SE2002/000043|   E04F|A floorboard and ...|\n",
      "| PCT/NO2010/000081|   E21B|Method and system...|\n",
      "| PCT/ES2002/000118|   E01F|The invention rel...|\n",
      "| PCT/IB2013/000890|   E21B|The present inven...|\n",
      "| PCT/EP2009/067481|   E04B|The present inven...|\n",
      "+------------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF=joinedDF.filter(joinedDF.mono_ipc=='E').select('Application_Number','qua_ipc','_c1');joinedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"_c1\", outputCol=\"words\",pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"removededWords\") # stopWords=yourownstopwords\n",
    "vectorizer = CountVectorizer(inputCol=\"removededWords\", outputCol=\"rawFeatures\")\n",
    "#idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\") # minDocFreq=2, TF小於2的就忽略\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2)\n",
    "pipeline = Pipeline(stages=[tokenizer,remover ,vectorizer, idf])\n",
    "model = pipeline.fit(joinedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "| vocabList|counts|\n",
      "+----------+------+\n",
      "|      said|1092.0|\n",
      "|       one|1088.0|\n",
      "| invention| 936.0|\n",
      "|     least| 859.0|\n",
      "|     means| 841.0|\n",
      "|   element| 808.0|\n",
      "|     first| 781.0|\n",
      "|    device| 745.0|\n",
      "|  provided| 636.0|\n",
      "|    member| 629.0|\n",
      "| comprises| 628.0|\n",
      "|comprising| 625.0|\n",
      "|   surface| 622.0|\n",
      "|    second| 587.0|\n",
      "|  position| 553.0|\n",
      "|       end| 543.0|\n",
      "|      part| 513.0|\n",
      "|   relates| 484.0|\n",
      "|       two| 469.0|\n",
      "|      wall| 456.0|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "total_counts = model.transform(joinedDF).select('rawFeatures').rdd.map(lambda row: row['rawFeatures'].toArray()).reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])\n",
    "vocabList = model.stages[2].vocabulary\n",
    "d = {'vocabList':vocabList,'counts':total_counts}\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).write.csv('hdfs:///data/L_words_Of_Bad_Jan14.csv',header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=model.transform(joinedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|num_of_sample|\n",
      "+-------------+\n",
      "|         1480|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf.createOrReplaceTempView(\"table1\")\n",
    "spark.sql('select count(*) as num_of_sample from table1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Application_Number',\n",
       " 'qua_ipc',\n",
       " '_c1',\n",
       " 'words',\n",
       " 'removededWords',\n",
       " 'rawFeatures',\n",
       " 'features']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfForTrain=spark.sql('select qua_ipc,rawFeatures, features from table1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|qua_ipc|         rawFeatures|            features|\n",
      "+-------+--------------------+--------------------+\n",
      "|   E21C|(7639,[1,8,10,14,...|(7639,[1,8,10,14,...|\n",
      "|   E05B|(7639,[0,1,2,3,4,...|(7639,[0,1,2,3,4,...|\n",
      "|   G06F|(7639,[0,2,4,5,10...|(7639,[0,2,4,5,10...|\n",
      "|   E02D|(7639,[0,1,2,3,8,...|(7639,[0,1,2,3,8,...|\n",
      "|   E02D|(7639,[1,3,7,15,1...|(7639,[1,3,7,15,1...|\n",
      "|   E01C|(7639,[2,8,11,12,...|(7639,[2,8,11,12,...|\n",
      "|   E21B|(7639,[1,7,8,10,1...|(7639,[1,7,8,10,1...|\n",
      "|   E05B|(7639,[0,1,2,3,5,...|(7639,[0,1,2,3,5,...|\n",
      "|   E21B|(7639,[1,2,3,5,6,...|(7639,[1,2,3,5,6,...|\n",
      "|   E21C|(7639,[8,11,16,26...|(7639,[8,11,16,26...|\n",
      "|   E05B|(7639,[6,13,14,36...|(7639,[6,13,14,36...|\n",
      "|   E21C|(7639,[7,10,11,15...|(7639,[7,10,11,15...|\n",
      "|   E21B|(7639,[3,7,12,15,...|(7639,[3,7,12,15,...|\n",
      "|   E21B|(7639,[6,9,10,11,...|(7639,[6,9,10,11,...|\n",
      "|   E05B|(7639,[6,13,14,32...|(7639,[6,13,14,32...|\n",
      "|   E04F|(7639,[1,3,4,6,10...|(7639,[1,3,4,6,10...|\n",
      "|   E21B|(7639,[0,4,10,11,...|(7639,[0,4,10,11,...|\n",
      "|   E01F|(7639,[0,1,2,4,5,...|(7639,[0,1,2,4,5,...|\n",
      "|   E21B|(7639,[0,2,8,10,1...|(7639,[0,2,8,10,1...|\n",
      "|   E04B|(7639,[2,4,5,6,8,...|(7639,[2,4,5,6,8,...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidfForTrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors # !!!!caution: not from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------------+\n",
      "|qua_ipc|         rawFeatures|            features|indexedLabel|\n",
      "+-------+--------------------+--------------------+------------+\n",
      "|   E21C|(7639,[1,8,10,14,...|(7639,[1,8,10,14,...|        14.0|\n",
      "|   E05B|(7639,[0,1,2,3,4,...|(7639,[0,1,2,3,4,...|         3.0|\n",
      "|   G06F|(7639,[0,2,4,5,10...|(7639,[0,2,4,5,10...|        42.0|\n",
      "|   E02D|(7639,[0,1,2,3,8,...|(7639,[0,1,2,3,8,...|         4.0|\n",
      "|   E02D|(7639,[1,3,7,15,1...|(7639,[1,3,7,15,1...|         4.0|\n",
      "+-------+--------------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelIndexer = StringIndexer(inputCol='qua_ipc',outputCol='indexedLabel').fit(tfidfForTrain)\n",
    "labelIndexer.transform(tfidfForTrain).show(5, True)\n",
    "stage1=labelIndexer.transform(tfidfForTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = stage1.randomSplit([0.6, 0.4])\n",
    "#trainingData.show(2,truncate=False)\n",
    "#testData.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "logr = LogisticRegression(featuresCol='features', labelCol='indexedLabel')\n",
    "stage2=logr.fit(trainingData).transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------------+--------------------+--------------------+----------+\n",
      "|qua_ipc|         rawFeatures|            features|indexedLabel|       rawPrediction|         probability|prediction|\n",
      "+-------+--------------------+--------------------+------------+--------------------+--------------------+----------+\n",
      "|   G06F|(7639,[0,2,4,5,10...|(7639,[0,2,4,5,10...|        42.0|[15.6640025456533...|[7.46096478311355...|      10.0|\n",
      "|   E02D|(7639,[0,1,2,3,8,...|(7639,[0,1,2,3,8,...|         4.0|[1.77767684816139...|[0.00230542461085...|      10.0|\n",
      "|   E21B|(7639,[1,7,8,10,1...|(7639,[1,7,8,10,1...|         0.0|[52.0229897849143...|[1.0,2.6766560717...|       0.0|\n",
      "|   E21B|(7639,[3,7,12,15,...|(7639,[3,7,12,15,...|         0.0|[39.7653993751308...|[0.99999999999395...|       0.0|\n",
      "|   E21B|(7639,[6,9,10,11,...|(7639,[6,9,10,11,...|         0.0|[52.0800113979501...|[0.99999999999999...|       0.0|\n",
      "|   E05B|(7639,[6,13,14,32...|(7639,[6,13,14,32...|         3.0|[-14.759699366231...|[4.91360304031870...|       3.0|\n",
      "|   E21B|(7639,[0,4,10,11,...|(7639,[0,4,10,11,...|         0.0|[32.7894998876437...|[0.99999999008463...|       0.0|\n",
      "|   E01F|(7639,[0,1,2,4,5,...|(7639,[0,1,2,4,5,...|        17.0|[15.5314053225411...|[0.75042430289094...|       0.0|\n",
      "|   E04G|(7639,[2,4,7,9,10...|(7639,[2,4,7,9,10...|        11.0|[12.4515389380917...|[0.00452199175885...|       6.0|\n",
      "|   E06B|(7639,[0,1,4,9,10...|(7639,[0,1,4,9,10...|         2.0|[-3.3033431012007...|[1.35350069937513...|       2.0|\n",
      "|   E04F|(7639,[0,1,3,4,5,...|(7639,[0,1,3,4,5,...|         5.0|[0.80486068135602...|[7.52630682918305...|       5.0|\n",
      "|   E02D|(7639,[0,1,2,3,4,...|(7639,[0,1,2,3,4,...|         4.0|[0.25900809221544...|[1.60346273539367...|      12.0|\n",
      "|   E04F|(7639,[4,5,6,8,9,...|(7639,[4,5,6,8,9,...|         5.0|[-2.6942295703053...|[3.80655763141517...|       1.0|\n",
      "|   E05B|(7639,[5,8,11,14,...|(7639,[5,8,11,14,...|         3.0|[11.9750531760996...|[1.34162422812507...|       3.0|\n",
      "|   E05B|(7639,[0,6,14,30,...|(7639,[0,6,14,30,...|         3.0|[-2.6728806228139...|[5.85350730477927...|       3.0|\n",
      "|   E21B|(7639,[1,2,9,10,2...|(7639,[1,2,9,10,2...|         0.0|[42.9613195052480...|[0.99999999999999...|       0.0|\n",
      "|   E21B|(7639,[26,31,32,3...|(7639,[26,31,32,3...|         0.0|[10.2162036509116...|[0.80265519253776...|       0.0|\n",
      "|   E04B|(7639,[2,5,6,10,1...|(7639,[2,5,6,10,1...|         1.0|[3.60328195088580...|[2.09515775121467...|       1.0|\n",
      "|   E21B|(7639,[1,2,4,15,1...|(7639,[1,2,4,15,1...|         0.0|[21.8988986377348...|[0.99999884488019...|       0.0|\n",
      "|   E21B|(7639,[7,21,26,35...|(7639,[7,21,26,35...|         0.0|[25.2543261396910...|[0.99978150042154...|       0.0|\n",
      "+-------+--------------------+--------------------+------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stage2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(probability=DenseVector([0.0007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9991, 0.0, 0.0, 0.0, 0.0, 0.0001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(probability=DenseVector([0.0023, 0.0, 0.0003, 0.0112, 0.0911, 0.1273, 0.0002, 0.003, 0.0013, 0.2226, 0.2468, 0.0003, 0.0004, 0.0008, 0.0, 0.0, 0.1197, 0.0008, 0.0, 0.0065, 0.0001, 0.0027, 0.0013, 0.0003, 0.0024, 0.1574, 0.0, 0.0001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0001, 0.0001, 0.0001, 0.0, 0.0, 0.0001, 0.0, 0.0001, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(probability=DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(probability=DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(probability=DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(probability=DenseVector([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(probability=DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(probability=DenseVector([0.7504, 0.0, 0.0762, 0.0, 0.1609, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0121, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(probability=DenseVector([0.0045, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])),\n",
       " Row(probability=DenseVector([0.0, 0.0, 0.9999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_log=stage2.select('probability').collect();prob_log[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------+\n",
      "|            features|qua_ipc|predictedLabel|\n",
      "+--------------------+-------+--------------+\n",
      "|(7639,[0,2,4,5,10...|   G06F|          E04C|\n",
      "|(7639,[0,1,2,3,8,...|   E02D|          E04C|\n",
      "|(7639,[1,7,8,10,1...|   E21B|          E21B|\n",
      "|(7639,[3,7,12,15,...|   E21B|          E21B|\n",
      "|(7639,[6,9,10,11,...|   E21B|          E21B|\n",
      "|(7639,[6,13,14,32...|   E05B|          E05B|\n",
      "|(7639,[0,4,10,11,...|   E21B|          E21B|\n",
      "|(7639,[0,1,2,4,5,...|   E01F|          E21B|\n",
      "|(7639,[2,4,7,9,10...|   E04G|          E03D|\n",
      "|(7639,[0,1,4,9,10...|   E06B|          E06B|\n",
      "|(7639,[0,1,3,4,5,...|   E04F|          E04F|\n",
      "|(7639,[0,1,2,3,4,...|   E02D|          E04D|\n",
      "|(7639,[4,5,6,8,9,...|   E04F|          E04B|\n",
      "|(7639,[5,8,11,14,...|   E05B|          E05B|\n",
      "|(7639,[0,6,14,30,...|   E05B|          E05B|\n",
      "|(7639,[1,2,9,10,2...|   E21B|          E21B|\n",
      "|(7639,[26,31,32,3...|   E21B|          E21B|\n",
      "|(7639,[2,5,6,10,1...|   E04B|          E04B|\n",
      "|(7639,[1,2,4,15,1...|   E21B|          E21B|\n",
      "|(7639,[7,21,26,35...|   E21B|          E21B|\n",
      "+--------------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",labels=labelIndexer.labels)\n",
    "labelConverter.transform(stage2).select('features','qua_ipc','predictedLabel').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter.transform(stage2).select('indexedLabel','prediction').createOrReplaceTempView('table2')\n",
    "predictionAndTarget = spark.sql('select indexedLabel as label, prediction from table2')\n",
    "\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "acc = evaluatorMulti.evaluate(predictionAndTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4483591662799168\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
