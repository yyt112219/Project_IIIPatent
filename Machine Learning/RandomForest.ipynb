{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"yarn\")\\\n",
    ".config('spark.executor.cores', '2')\\\n",
    ".config('spark.executor.memory', '9G')\\\n",
    ".config('spark.executor.instances','10')\\\n",
    ".appName('bdse62')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract=spark.read.csv('hdfs:///bdse71/ABS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = spark.read.json('hdfs:///data/df_mono_ipc.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+----------------+------------------+--------------------+--------------+--------+\n",
      "|              Agents|          Applicants|Application_Number|   Designated_States|           Inventors|  PublicationNo_Name|Publication_Date|Publication_Number|               Title|ipc_simplified|mono_ipc|\n",
      "+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+----------------+------------------+--------------------+--------------+--------+\n",
      "|'ОСИПОВА, Наталья...|[' [CY]/[CY] (All...| PCT/IB2011/002779|[[BF,  BJ,  CF,  ...|'ЯСНЕЦОВ, Владими...|1. WO2012028965 -...|      08-03-2012|    WO/2012/028965|[EN), COMBINATION...|           [A]|       A|\n",
      "|                 nan|[' [BY]/[BY] (AM,...| PCT/BY2015/000005|[[BF,  BJ,  CF,  ...|'ЖАВНЕРКО, Геннад...|1. WO2017070769 -...|      04-05-2017|    WO/2017/070769|[EN), COMPOSITE O...|        [C, G]|       C|\n",
      "|                 nan|[' [BY]/[BY] (AM,...| PCT/BY2015/000005|[[BF,  BJ,  CF,  ...|'ЖАВНЕРКО, Геннад...|1. WO2017070769 -...|      04-05-2017|    WO/2017/070769|[EN), COMPOSITE O...|        [C, G]|       G|\n",
      "|\"ООО 'MEЖДУНAРOДН...|[' [RU]/[RU] (All...| PCT/EA2007/000006|[[BF,  BJ,  CF,  ...|'ТАХАУТДИНОВ Шафа...|1. WO2008046426 -...|      24-04-2008|    WO/2008/046426|[EN), METHOD FOR ...|           [B]|       B|\n",
      "|                 nan|[' [AZ]/[AZ]': 'Б...| PCT/AZ2017/000007|[[BF,  BJ,  CF,  ...|'АЛЫЕВ, Абульфат ...|1. WO2019056075 -...|      28-03-2019|    WO/2019/056075|[EN), METHOD FOR ...|        [C, G]|       C|\n",
      "|                 nan|[' [AZ]/[AZ]': 'Б...| PCT/AZ2017/000007|[[BF,  BJ,  CF,  ...|'АЛЫЕВ, Абульфат ...|1. WO2019056075 -...|      28-03-2019|    WO/2019/056075|[EN), METHOD FOR ...|        [C, G]|       G|\n",
      "|'BENEDETTO, Marco...|[' [IT]/[IT]': 'C...| PCT/IB2015/056276|[[BF,  BJ,  CF,  ...|'MOGNA, Giovanni'...|1. WO2016027231 -...|      25-02-2016|    WO/2016/027231|[EN), METHOD FOR ...|           [A]|       A|\n",
      "|                 nan|[' [BY]/[BY] (AL,...| PCT/BY2014/000007|[[BF,  BJ,  CF,  ...|'ШИРИПОВ, Владими...|1. WO2016033674 -...|      10-03-2016|    WO/2016/033674|[EN), DEVICE FOR ...|           [B]|       B|\n",
      "|                 nan|[' [US]/[US] (All...| PCT/IB2009/050270|[[BF,  BJ,  CF,  ...|'ИВАЩЕНКО Андрей ...|1. WO2009093206 -...|      30-07-2009|    WO/2009/093206|[EN), 3-SULFONYL-...|        [C, A]|       C|\n",
      "|                 nan|[' [US]/[US] (All...| PCT/IB2009/050270|[[BF,  BJ,  CF,  ...|'ИВАЩЕНКО Андрей ...|1. WO2009093206 -...|      30-07-2009|    WO/2009/093206|[EN), 3-SULFONYL-...|        [C, A]|       A|\n",
      "|                 nan|[' [DE]/[DE] (All...| PCT/EP2011/062178|[[BF,  BJ,  CF,  ...|'FISCHER, Rüdiger...|1. WO2012010525 -...|      26-01-2012|    WO/2012/010525|[DE), VERWENDUNG ...|           [A]|       A|\n",
      "|                 nan|[' [AZ]/[AZ] (All...| PCT/AZ2008/000002|[[BF,  BJ,  CF,  ...|'ГАШИМОВ, Ариф Ма...|1. WO2009105840 -...|      03-09-2009|    WO/2009/105840|[EN), METHOD FOR ...|           [H]|       H|\n",
      "|'HOFFMANN EITLE S...|[' [IT]/[IT]': 'A...| PCT/IB2018/058051|[[BF,  BJ,  CF,  ...|  'FERRARI, Alessio'|1. WO2019077521 -...|      25-04-2019|    WO/2019/077521|[EN), LIQUID COMP...|           [A]|       A|\n",
      "|'БАЗАНОВ, Юрий Бо...|[' [RU]/[RU]': 'О...| PCT/IB2015/050897|[[BF,  BJ,  CF,  ...|'ЕРИМБЕТОВ, Кенес...|1. WO2015118488 -...|      13-08-2015|    WO/2015/118488|[EN), AGENT FOR T...|           [A]|       A|\n",
      "|'DR. LASSE WEINMA...|[' [DE]/[DE]': 'J...| PCT/EP2016/072524|[[BF,  BJ,  CF,  ...|'HUDECEK, Michael...|1. WO2017050884 -...|      30-03-2017|    WO/2017/050884|[EN), A METHOD FO...|        [C, A]|       C|\n",
      "|'DR. LASSE WEINMA...|[' [DE]/[DE]': 'J...| PCT/EP2016/072524|[[BF,  BJ,  CF,  ...|'HUDECEK, Michael...|1. WO2017050884 -...|      30-03-2017|    WO/2017/050884|[EN), A METHOD FO...|        [C, A]|       A|\n",
      "|'ФЕДОРОВ, Дмитрий...|[' [RU]/[RU] (All...| PCT/IB2012/052483|[[BF,  BJ,  CF,  ...|'ШУРИГИН, Михаил ...|1. WO2012156938 -...|      22-11-2012|    WO/2012/156938|[EN), COMPOUNDS P...|        [C, A]|       C|\n",
      "|'ФЕДОРОВ, Дмитрий...|[' [RU]/[RU] (All...| PCT/IB2012/052483|[[BF,  BJ,  CF,  ...|'ШУРИГИН, Михаил ...|1. WO2012156938 -...|      22-11-2012|    WO/2012/156938|[EN), COMPOUNDS P...|        [C, A]|       A|\n",
      "|'ASTRAZENECA INTE...|[' [SE]/[SE] (AE,...| PCT/GB2010/050822|[[BF,  BJ,  CF,  ...|'BENNETT, Nichola...|1. WO2010133882 -...|      25-11-2010|    WO/2010/133882|[EN), DISACCHARIN...|        [C, A]|       C|\n",
      "|'ASTRAZENECA INTE...|[' [SE]/[SE] (AE,...| PCT/GB2010/050822|[[BF,  BJ,  CF,  ...|'BENNETT, Nichola...|1. WO2010133882 -...|      25-11-2010|    WO/2010/133882|[EN), DISACCHARIN...|        [C, A]|       A|\n",
      "+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+----------------+------------------+--------------------+--------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|              _c0|                 _c1|\n",
      "+-----------------+--------------------+\n",
      "|PCT/EP2001/009832|Liquid crystal mi...|\n",
      "|PCT/KR2000/000814|The present inven...|\n",
      "|PCT/IL2000/000667|The present inven...|\n",
      "|PCT/DK2001/000750|The invention rel...|\n",
      "|PCT/EP2001/012972|The invention rel...|\n",
      "|PCT/EP2007/000257|The invention rel...|\n",
      "|PCT/RU2001/000190|The inventive met...|\n",
      "|PCT/IL2000/000459|The present inven...|\n",
      "|PCT/KR2006/002440|A diagnostic kit ...|\n",
      "|PCT/KR2001/001941|The present inven...|\n",
      "|PCT/SE2002/000250|To facilitate eg ...|\n",
      "|PCT/DE2001/001579|The invention rel...|\n",
      "|PCT/IL2001/000088|A method and circ...|\n",
      "|PCT/KR2001/000965|A preferable embo...|\n",
      "|PCT/GB2001/002487|New spisulosine d...|\n",
      "|PCT/HU2001/000068|A system of appar...|\n",
      "|PCT/US2000/028670|Novel antigenpres...|\n",
      "|PCT/GB2001/002151|A compound of for...|\n",
      "|PCT/DK2000/000496|The present inven...|\n",
      "|PCT/US2000/025466|A method and appa...|\n",
      "+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abstract.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join two tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression = testDF['Application_Number']==abstract[\"_c0\"]\n",
    "joinType = \"inner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|Application_Number|mono_ipc|                 _c1|\n",
      "+------------------+--------+--------------------+\n",
      "| PCT/AT2002/000007|       B|The invention rel...|\n",
      "| PCT/AU2001/001240|       A|An electrode asse...|\n",
      "| PCT/CH2000/000616|       A|The invention rel...|\n",
      "| PCT/DE2001/002185|       G|The invention rel...|\n",
      "| PCT/DE2001/003336|       G|According to the ...|\n",
      "| PCT/DE2001/003917|       H|The invention rel...|\n",
      "| PCT/EP2001/003772|       C|The invention rel...|\n",
      "| PCT/EP2001/004360|       C|A method for dete...|\n",
      "| PCT/EP2001/005589|       F|With a view to ra...|\n",
      "| PCT/EP2001/005723|       C|The invention rel...|\n",
      "| PCT/EP2001/005780|       H|The invention rel...|\n",
      "| PCT/EP2001/005780|       C|The invention rel...|\n",
      "| PCT/FI2001/000982|       C|This invention re...|\n",
      "| PCT/FR2000/002789|       C|The invention rel...|\n",
      "| PCT/FR2000/002789|       A|The invention rel...|\n",
      "| PCT/IB2001/000244|       C|The present inven...|\n",
      "| PCT/IB2001/001581|       C|The present inven...|\n",
      "| PCT/IB2001/001581|       A|The present inven...|\n",
      "| PCT/IL2000/000357|       G|Methods of creati...|\n",
      "| PCT/AT2001/000202|       A|According to the ...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF=testDF.join(abstract,joinExpression,joinType).select('Application_Number','mono_ipc','_c1')\n",
    "joinedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Codec [true] is not available. Known codecs are bzip2, deflate, uncompressed, lz4, gzip, snappy, none.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-871f5406fa61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoinedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hdfs:///data/mergedData.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, mode, compression, dateFormat, timestampFormat, lineSep, encoding, ignoreNullFields)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdateFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdateFormat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestampFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimestampFormat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m             lineSep=lineSep, encoding=encoding, ignoreNullFields=ignoreNullFields)\n\u001b[0;32m--> 909\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Codec [true] is not available. Known codecs are bzip2, deflate, uncompressed, lz4, gzip, snappy, none."
     ]
    }
   ],
   "source": [
    "joinedDF.write.json('hdfs:///data/mergedData.json',compression='bzip2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"_c1\", outputCol=\"words\",pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"removededWords\") # stopWords=yourownstopwords\n",
    "vectorizer = CountVectorizer(inputCol=\"removededWords\", outputCol=\"rawFeatures\")\n",
    "#idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\") # minDocFreq=2, TF小於2的就忽略\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2)\n",
    "pipeline = Pipeline(stages=[tokenizer,remover ,vectorizer, idf])\n",
    "model = pipeline.fit(joinedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "| vocabList|counts|\n",
      "+----------+------+\n",
      "| invention|3494.0|\n",
      "|      said|2348.0|\n",
      "|       one|2238.0|\n",
      "|     least|1949.0|\n",
      "|   relates|1886.0|\n",
      "|    method|1742.0|\n",
      "|comprising|1563.0|\n",
      "|    device|1280.0|\n",
      "|     means|1267.0|\n",
      "| comprises|1026.0|\n",
      "|     first|1022.0|\n",
      "|   surface| 961.0|\n",
      "|    system| 955.0|\n",
      "|       use| 946.0|\n",
      "|      also| 942.0|\n",
      "|  material| 910.0|\n",
      "|    second| 910.0|\n",
      "|  provided| 893.0|\n",
      "|   present| 833.0|\n",
      "|   wherein| 811.0|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "total_counts = model.transform(joinedDF).select('rawFeatures').rdd.map(lambda row: row['rawFeatures'].toArray()).reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])\n",
    "vocabList = model.stages[2].vocabulary\n",
    "d = {'vocabList':vocabList,'counts':total_counts}\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).write.csv('hdfs:///data/words_Of_Bad_Jan12.csv',header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=model.transform(joinedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    4091|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf.createOrReplaceTempView(\"table1\")\n",
    "spark.sql('select count(*) as num_of_sample from table1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Application_Number',\n",
       " 'mono_ipc',\n",
       " '_c1',\n",
       " 'words',\n",
       " 'removededWords',\n",
       " 'rawFeatures',\n",
       " 'features']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfForTrain=spark.sql('select mono_ipc,rawFeatures, features from table1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|mono_ipc|         rawFeatures|            features|\n",
      "+--------+--------------------+--------------------+\n",
      "|       B|(17518,[0,3,4,6,7...|(17518,[0,3,4,6,7...|\n",
      "|       A|(17518,[6,29,95,1...|(17518,[6,29,95,1...|\n",
      "|       A|(17518,[0,3,4,5,1...|(17518,[0,3,4,5,1...|\n",
      "|       G|(17518,[0,4,5,14,...|(17518,[0,4,5,14,...|\n",
      "|       G|(17518,[0,1,7,15,...|(17518,[0,1,7,15,...|\n",
      "|       H|(17518,[0,3,4,5,8...|(17518,[0,3,4,5,8...|\n",
      "|       C|(17518,[0,1,4,5,2...|(17518,[0,1,4,5,2...|\n",
      "|       C|(17518,[5,6,10,16...|(17518,[5,6,10,16...|\n",
      "|       F|(17518,[1,17,71,8...|(17518,[1,17,71,8...|\n",
      "|       C|(17518,[0,2,4,19,...|(17518,[0,2,4,19,...|\n",
      "|       H|(17518,[0,2,3,4,5...|(17518,[0,2,3,4,5...|\n",
      "|       C|(17518,[0,2,3,4,5...|(17518,[0,2,3,4,5...|\n",
      "|       C|(17518,[0,4,5,15,...|(17518,[0,4,5,15,...|\n",
      "|       C|(17518,[0,2,3,4,6...|(17518,[0,2,3,4,6...|\n",
      "|       A|(17518,[0,2,3,4,6...|(17518,[0,2,3,4,6...|\n",
      "|       C|(17518,[0,4,14,18...|(17518,[0,4,14,18...|\n",
      "|       C|(17518,[0,1,4,5,6...|(17518,[0,1,4,5,6...|\n",
      "|       A|(17518,[0,1,4,5,6...|(17518,[0,1,4,5,6...|\n",
      "|       G|(17518,[2,3,5,11,...|(17518,[2,3,5,11,...|\n",
      "|       A|(17518,[0,2,6,7,9...|(17518,[0,2,6,7,9...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidfForTrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    4091|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidfForTrain.createOrReplaceTempView('table3')\n",
    "spark.sql('select count(*) as  from table3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors # !!!!caution: not from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+------------+\n",
      "|mono_ipc|         rawFeatures|            features|indexedLabel|\n",
      "+--------+--------------------+--------------------+------------+\n",
      "|       B|(17518,[0,3,4,6,7...|(17518,[0,3,4,6,7...|         2.0|\n",
      "|       A|(17518,[6,29,95,1...|(17518,[6,29,95,1...|         1.0|\n",
      "|       A|(17518,[0,3,4,5,1...|(17518,[0,3,4,5,1...|         1.0|\n",
      "|       G|(17518,[0,4,5,14,...|(17518,[0,4,5,14,...|         3.0|\n",
      "|       G|(17518,[0,1,7,15,...|(17518,[0,1,7,15,...|         3.0|\n",
      "+--------+--------------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelIndexer = StringIndexer(inputCol='mono_ipc',outputCol='indexedLabel').fit(tfidfForTrain)\n",
    "labelIndexer.transform(tfidfForTrain).show(5, True)\n",
    "stage1=labelIndexer.transform(tfidfForTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = stage1.randomSplit([0.6, 0.4])\n",
    "#trainingData.show(2,truncate=False)\n",
    "#testData.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "RandomForestClassifier(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    predictionCol='prediction',\n",
    "    probabilityCol='probability',\n",
    "    rawPredictionCol='rawPrediction',\n",
    "    maxDepth=5,\n",
    "    maxBins=32,\n",
    "    minInstancesPerNode=1,\n",
    "    minInfoGain=0.0,\n",
    "    maxMemoryInMB=256,\n",
    "    cacheNodeIds=False,\n",
    "    checkpointInterval=10,\n",
    "    impurity='gini',\n",
    "    numTrees=20,\n",
    "    featureSubsetStrategy='auto',\n",
    "    seed=None,\n",
    "    subsamplingRate=1.0,\n",
    "    leafCol='',\n",
    "    minWeightFractionPerNode=0.0,\n",
    "    weightCol=None,\n",
    "    bootstrap=True,\n",
    ")\n",
    "Docstring:     \n",
    "`Random Forest <http://en.wikipedia.org/wiki/Random_forest>`_\n",
    "learning algorithm for classification.\n",
    "It supports both binary and multiclass labels, as well as both continuous and categorical```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'indexedLabel', maxDepth=30, maxBins=17518)\n",
    "rfModel = rf.fit(trainingData).transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+------------+--------------------+--------------------+----------+\n",
      "|mono_ipc|         rawFeatures|            features|indexedLabel|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+------------+--------------------+--------------------+----------+\n",
      "|       A|(17518,[0,3,4,5,1...|(17518,[0,3,4,5,1...|         1.0|[5.31949626363017...|[0.26597481318150...|       1.0|\n",
      "|       C|(17518,[0,1,4,5,6...|(17518,[0,1,4,5,6...|         0.0|[5.75319723618016...|[0.28765986180900...|       1.0|\n",
      "|       C|(17518,[0,1,4,5,2...|(17518,[0,1,4,5,2...|         0.0|[6.65036692082716...|[0.33251834604135...|       0.0|\n",
      "|       C|(17518,[0,2,3,4,6...|(17518,[0,2,3,4,6...|         0.0|[6.63741379616861...|[0.33187068980843...|       0.0|\n",
      "|       C|(17518,[0,2,4,19,...|(17518,[0,2,4,19,...|         0.0|[9.13415339567911...|[0.45670766978395...|       0.0|\n",
      "|       C|(17518,[0,4,14,18...|(17518,[0,4,14,18...|         0.0|[6.54433759955227...|[0.32721687997761...|       0.0|\n",
      "|       C|(17518,[5,6,10,16...|(17518,[5,6,10,16...|         0.0|[5.07703623162633...|[0.25385181158131...|       1.0|\n",
      "|       G|(17518,[0,4,5,14,...|(17518,[0,4,5,14,...|         3.0|[4.91653395715250...|[0.24582669785762...|       0.0|\n",
      "|       H|(17518,[0,2,3,4,5...|(17518,[0,2,3,4,5...|         4.0|[5.19450793496537...|[0.25972539674826...|       1.0|\n",
      "|       A|(17518,[0,4,42,52...|(17518,[0,4,42,52...|         1.0|[5.72797689348574...|[0.28639884467428...|       0.0|\n",
      "|       A|(17518,[0,30,37,3...|(17518,[0,30,37,3...|         1.0|[5.97870125960434...|[0.29893506298021...|       0.0|\n",
      "|       C|(17518,[0,1,4,19,...|(17518,[0,1,4,19,...|         0.0|[8.99724089197699...|[0.44986204459884...|       0.0|\n",
      "|       C|(17518,[1,5,6,9,1...|(17518,[1,5,6,9,1...|         0.0|[6.31206593622667...|[0.31560329681133...|       0.0|\n",
      "|       G|(17518,[0,1,4,7,8...|(17518,[0,1,4,7,8...|         3.0|[4.21274303418709...|[0.21063715170935...|       1.0|\n",
      "|       G|(17518,[2,5,10,14...|(17518,[2,5,10,14...|         3.0|[5.01149979691262...|[0.25057498984563...|       1.0|\n",
      "|       B|(17518,[1,2,8,23,...|(17518,[1,2,8,23,...|         2.0|[4.91070870924298...|[0.24553543546214...|       1.0|\n",
      "|       C|(17518,[0,1,2,4,5...|(17518,[0,1,2,4,5...|         0.0|[5.54316247341693...|[0.27715812367084...|       1.0|\n",
      "|       C|(17518,[1,2,3,6,1...|(17518,[1,2,3,6,1...|         0.0|[4.71221997668394...|[0.23561099883419...|       1.0|\n",
      "|       C|(17518,[2,20,22,2...|(17518,[2,20,22,2...|         0.0|[9.42590046750793...|[0.47129502337539...|       0.0|\n",
      "|       G|(17518,[0,5,7,14,...|(17518,[0,5,7,14,...|         3.0|[5.08070946260271...|[0.25403547313013...|       1.0|\n",
      "+--------+--------------------+--------------------+------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfModel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(probability=DenseVector([0.266, 0.2867, 0.1436, 0.1186, 0.0745, 0.055, 0.0398, 0.0158])),\n",
       " Row(probability=DenseVector([0.2877, 0.3246, 0.1246, 0.1034, 0.0664, 0.0466, 0.033, 0.0138])),\n",
       " Row(probability=DenseVector([0.3325, 0.2861, 0.123, 0.1008, 0.0634, 0.0467, 0.0337, 0.0138])),\n",
       " Row(probability=DenseVector([0.3319, 0.2657, 0.1301, 0.1028, 0.067, 0.0488, 0.0383, 0.0156])),\n",
       " Row(probability=DenseVector([0.4567, 0.3167, 0.0726, 0.0575, 0.0395, 0.0265, 0.0192, 0.0114])),\n",
       " Row(probability=DenseVector([0.3272, 0.3024, 0.1209, 0.0995, 0.0606, 0.0443, 0.0316, 0.0134])),\n",
       " Row(probability=DenseVector([0.2539, 0.267, 0.131, 0.1718, 0.0678, 0.048, 0.0425, 0.0181])),\n",
       " Row(probability=DenseVector([0.2458, 0.2331, 0.1343, 0.1853, 0.0929, 0.0541, 0.042, 0.0126])),\n",
       " Row(probability=DenseVector([0.2597, 0.2826, 0.1488, 0.121, 0.075, 0.0557, 0.0413, 0.0159])),\n",
       " Row(probability=DenseVector([0.2864, 0.2806, 0.1423, 0.1137, 0.0708, 0.0516, 0.0381, 0.0165]))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_RF=rfModel.select('probability').collect();prob_RF[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------+\n",
      "|            features|mono_ipc|predictedLabel|\n",
      "+--------------------+--------+--------------+\n",
      "|(17518,[0,3,4,5,1...|       A|             A|\n",
      "|(17518,[0,1,4,5,6...|       C|             A|\n",
      "|(17518,[0,1,4,5,2...|       C|             C|\n",
      "|(17518,[0,2,3,4,6...|       C|             C|\n",
      "|(17518,[0,2,4,19,...|       C|             C|\n",
      "|(17518,[0,4,14,18...|       C|             C|\n",
      "|(17518,[5,6,10,16...|       C|             A|\n",
      "|(17518,[0,4,5,14,...|       G|             C|\n",
      "|(17518,[0,2,3,4,5...|       H|             A|\n",
      "|(17518,[0,4,42,52...|       A|             C|\n",
      "|(17518,[0,30,37,3...|       A|             C|\n",
      "|(17518,[0,1,4,19,...|       C|             C|\n",
      "|(17518,[1,5,6,9,1...|       C|             C|\n",
      "|(17518,[0,1,4,7,8...|       G|             A|\n",
      "|(17518,[2,5,10,14...|       G|             A|\n",
      "|(17518,[1,2,8,23,...|       B|             A|\n",
      "|(17518,[0,1,2,4,5...|       C|             A|\n",
      "|(17518,[1,2,3,6,1...|       C|             A|\n",
      "|(17518,[2,20,22,2...|       C|             C|\n",
      "|(17518,[0,5,7,14,...|       G|             A|\n",
      "+--------------------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",labels=labelIndexer.labels)\n",
    "labelConverter.transform(rfModel).select('features','mono_ipc','predictedLabel').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter.transform(rfModel).select('indexedLabel','prediction').createOrReplaceTempView('table2')\n",
    "predictionAndTarget = spark.sql('select indexedLabel as label, prediction from table2')\n",
    "\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "acc = evaluatorMulti.evaluate(predictionAndTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.275723232216577\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
