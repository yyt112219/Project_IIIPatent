{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"yarn\")\\\n",
    ".config('spark.executor.cores', '2')\\\n",
    ".config('spark.executor.memory', '7G')\\\n",
    ".config('spark.driver.memory','4G')\\\n",
    ".config('spark.executor.instances','8')\\\n",
    ".appName('bdse62')\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract=spark.read.csv('hdfs:///bdse71/ABS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = spark.read.parquet('hdfs:///data/jso_hierarchy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+-------+-------+\n",
      "|Application_Number|mono_ipc|tri_ipc|qua_ipc|\n",
      "+------------------+--------+-------+-------+\n",
      "| PCT/CA2018/000243|       B|    A61|   B25J|\n",
      "| PCT/CA2019/050200|       G|    G02|   G02C|\n",
      "| PCT/CL2016/050019|       G|    G06|   G06Q|\n",
      "| PCT/CN2012/070253|       A|    A43|   A43B|\n",
      "| PCT/DE2014/100160|       F|    F41|   F41H|\n",
      "| PCT/DE2016/100332|       C|    C10|   C10B|\n",
      "| PCT/DK2005/000435|       A|    A61|   A61K|\n",
      "| PCT/EP1999/004038|       E|    E21|   E21B|\n",
      "| PCT/EP1999/008598|       A|    A61|   A61K|\n",
      "| PCT/EP2000/004724|       A|    A23|   A23G|\n",
      "| PCT/EP2000/011562|       C|    C11|   C11D|\n",
      "| PCT/EP2001/002279|       C|    A01|   A01N|\n",
      "| PCT/EP2001/003614|       B|    F16|   F16B|\n",
      "| PCT/EP2001/003789|       A|    A22|   A22B|\n",
      "| PCT/EP2001/010090|       C|    C12|   C12N|\n",
      "| PCT/EP2001/013071|       C|    C07|   C08F|\n",
      "| PCT/EP2001/014943|       B|    B60|   B60T|\n",
      "| PCT/EP2003/007473|       C|    A01|   A01N|\n",
      "| PCT/EP2003/011110|       A|    A61|   A61K|\n",
      "| PCT/EP2005/006018|       C|    A01|   A01N|\n",
      "+------------------+--------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|              _c0|                 _c1|\n",
      "+-----------------+--------------------+\n",
      "|PCT/IB2015/050868|For the productio...|\n",
      "|PCT/EP2013/003311|A composition com...|\n",
      "|PCT/AU2010/000148|An agricultural a...|\n",
      "|PCT/CN2013/072341|A detection metho...|\n",
      "|PCT/EP2013/058979|The invention rel...|\n",
      "|PCT/EP2016/063487|The present inven...|\n",
      "|PCT/GB2009/001774|A compound having...|\n",
      "|PCT/EP2013/077705|A compound with t...|\n",
      "|PCT/EP2009/006204|The invention rel...|\n",
      "|PCT/KR2017/010450|The present inven...|\n",
      "|PCT/EP2010/000270|The present inven...|\n",
      "|PCT/EP2013/077695|A compound of for...|\n",
      "|PCT/KR2012/004595|The present funct...|\n",
      "|PCT/JP2015/004803|A zoom lens accor...|\n",
      "|PCT/AT2017/000070|The invention rel...|\n",
      "|PCT/KR2015/006148|The present inven...|\n",
      "|PCT/IB2014/001029|A conjugate of fo...|\n",
      "|PCT/GB2012/052526|A method for prom...|\n",
      "|PCT/EP2001/009832|Liquid crystal mi...|\n",
      "|PCT/KR2000/000814|The present inven...|\n",
      "+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abstract=abstract.dropna()\n",
    "abstract.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join two tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "joinExpression = testDF['Application_Number']==abstract[\"_c0\"]\n",
    "joinType = \"inner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+--------------------+\n",
      "|Application_Number|mono_ipc|                 _c1|\n",
      "+------------------+--------+--------------------+\n",
      "| PCT/EP2010/000270|       A|The present inven...|\n",
      "| PCT/EP2013/077695|       C|A compound of for...|\n",
      "| PCT/EP2001/009832|       C|Liquid crystal mi...|\n",
      "| PCT/KR2000/000814|       F|The present inven...|\n",
      "| PCT/BR2010/000175|       C|The organic matte...|\n",
      "| PCT/IL2000/000667|       H|The present inven...|\n",
      "| PCT/KR2009/007459|       A|The present inven...|\n",
      "| PCT/DK2001/000750|       C|The invention rel...|\n",
      "| PCT/EP2008/009031|       A|The invention rel...|\n",
      "| PCT/EP2001/012972|       B|The invention rel...|\n",
      "| PCT/EP2007/000257|       B|The invention rel...|\n",
      "| PCT/EP2003/014296|       C|The present inven...|\n",
      "| PCT/GB2003/005261|       C|Fungicidal compou...|\n",
      "| PCT/EP2013/066528|       B|The present inven...|\n",
      "| PCT/RU2001/000190|       G|The inventive met...|\n",
      "| PCT/IL2000/000459|       C|The present inven...|\n",
      "| PCT/KR2006/002440|       B|A diagnostic kit ...|\n",
      "| PCT/KR2001/001941|       F|The present inven...|\n",
      "| PCT/SE2002/000250|       H|To facilitate eg ...|\n",
      "| PCT/DE2001/001579|       G|The invention rel...|\n",
      "+------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinedDF=testDF.join(abstract,joinExpression,joinType).select('Application_Number','mono_ipc','_c1')\n",
    "joinedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"_c1\", outputCol=\"words\",pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"removededWords\") # stopWords=yourownstopwords\n",
    "vectorizer = CountVectorizer(inputCol=\"removededWords\", outputCol=\"rawFeatures\")\n",
    "#idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\") # minDocFreq=2, TF小於2的就忽略\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=2)\n",
    "pipeline = Pipeline(stages=[tokenizer,remover ,vectorizer, idf])\n",
    "model = pipeline.fit(joinedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 1212 tasks (1025.5 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d6b5bd599321>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'B'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtotal_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoinedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoinedDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmono_ipc\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rawFeatures'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rawFeatures'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvocabList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'vocabList'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvocabList\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'counts'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtotal_counts\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \"\"\"\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 1212 tasks (1025.5 MiB) is bigger than spark.driver.maxResultSize (1024.0 MiB)\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:168)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in ['A','B','C','D','E','F','G','H']:\n",
    "    total_counts = model.transform(joinedDF.filter(joinedDF.mono_ipc==i)).select('rawFeatures').rdd.map(lambda row: row['rawFeatures'].toArray()).reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])\n",
    "    vocabList = model.stages[2].vocabulary\n",
    "    d = {'vocabList':vocabList,'counts':total_counts}\n",
    "    spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).limit(200).sort('counts',ascending=False).write.csv(f'hdfs:///data/wob_{i}',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "| vocabList|counts|\n",
      "+----------+------+\n",
      "| invention|3494.0|\n",
      "|      said|2348.0|\n",
      "|       one|2238.0|\n",
      "|     least|1949.0|\n",
      "|   relates|1886.0|\n",
      "|    method|1742.0|\n",
      "|comprising|1563.0|\n",
      "|    device|1280.0|\n",
      "|     means|1267.0|\n",
      "| comprises|1026.0|\n",
      "|     first|1022.0|\n",
      "|   surface| 961.0|\n",
      "|    system| 955.0|\n",
      "|       use| 946.0|\n",
      "|      also| 942.0|\n",
      "|  material| 910.0|\n",
      "|    second| 910.0|\n",
      "|  provided| 893.0|\n",
      "|   present| 833.0|\n",
      "|   wherein| 811.0|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "total_counts = model.transform(joinedDF.filter(joinedDF.mono_ipc=='A')).select('rawFeatures').rdd.map(lambda row: row['rawFeatures'].toArray()).reduce(lambda x,y: [x[i]+y[i] for i in range(len(y))])\n",
    "vocabList = model.stages[2].vocabulary\n",
    "d = {'vocabList':vocabList,'counts':total_counts}\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).write.csv('hdfs:///data/words_Of_Bad_Jan12.csv',header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=model.transform(joinedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|num_of_sample|\n",
      "+-------------+\n",
      "|         4091|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf.createOrReplaceTempView(\"table1\")\n",
    "spark.sql('select count(*) as num_of_sample from table1').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Application_Number',\n",
       " 'mono_ipc',\n",
       " '_c1',\n",
       " 'words',\n",
       " 'removededWords',\n",
       " 'rawFeatures',\n",
       " 'features']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfForTrain=spark.sql('select mono_ipc,rawFeatures, features from table1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|mono_ipc|         rawFeatures|            features|\n",
      "+--------+--------------------+--------------------+\n",
      "|       B|(17518,[0,3,4,6,7...|(17518,[0,3,4,6,7...|\n",
      "|       A|(17518,[6,29,95,1...|(17518,[6,29,95,1...|\n",
      "|       A|(17518,[0,3,4,5,1...|(17518,[0,3,4,5,1...|\n",
      "|       G|(17518,[0,4,5,14,...|(17518,[0,4,5,14,...|\n",
      "|       G|(17518,[0,1,7,15,...|(17518,[0,1,7,15,...|\n",
      "|       H|(17518,[0,3,4,5,8...|(17518,[0,3,4,5,8...|\n",
      "|       C|(17518,[0,1,4,5,2...|(17518,[0,1,4,5,2...|\n",
      "|       C|(17518,[5,6,10,16...|(17518,[5,6,10,16...|\n",
      "|       F|(17518,[1,17,71,8...|(17518,[1,17,71,8...|\n",
      "|       C|(17518,[0,2,4,19,...|(17518,[0,2,4,19,...|\n",
      "|       H|(17518,[0,2,3,4,5...|(17518,[0,2,3,4,5...|\n",
      "|       C|(17518,[0,2,3,4,5...|(17518,[0,2,3,4,5...|\n",
      "|       C|(17518,[0,4,5,15,...|(17518,[0,4,5,15,...|\n",
      "|       C|(17518,[0,2,3,4,6...|(17518,[0,2,3,4,6...|\n",
      "|       A|(17518,[0,2,3,4,6...|(17518,[0,2,3,4,6...|\n",
      "|       C|(17518,[0,4,14,18...|(17518,[0,4,14,18...|\n",
      "|       C|(17518,[0,1,4,5,6...|(17518,[0,1,4,5,6...|\n",
      "|       A|(17518,[0,1,4,5,6...|(17518,[0,1,4,5,6...|\n",
      "|       G|(17518,[2,3,5,11,...|(17518,[2,3,5,11,...|\n",
      "|       A|(17518,[0,2,6,7,9...|(17518,[0,2,6,7,9...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidfForTrain.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors # !!!!caution: not from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+------------+\n",
      "|mono_ipc|         rawFeatures|            features|indexedLabel|\n",
      "+--------+--------------------+--------------------+------------+\n",
      "|       B|(17518,[0,3,4,6,7...|(17518,[0,3,4,6,7...|         2.0|\n",
      "|       A|(17518,[6,29,95,1...|(17518,[6,29,95,1...|         1.0|\n",
      "|       A|(17518,[0,3,4,5,1...|(17518,[0,3,4,5,1...|         1.0|\n",
      "|       G|(17518,[0,4,5,14,...|(17518,[0,4,5,14,...|         3.0|\n",
      "|       G|(17518,[0,1,7,15,...|(17518,[0,1,7,15,...|         3.0|\n",
      "+--------+--------------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelIndexer = StringIndexer(inputCol='mono_ipc',outputCol='indexedLabel').fit(tfidfForTrain)\n",
    "labelIndexer.transform(tfidfForTrain).show(5, True)\n",
    "stage1=labelIndexer.transform(tfidfForTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = stage1.randomSplit([0.6, 0.4])\n",
    "#trainingData.show(2,truncate=False)\n",
    "#testData.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "layers = [17518, 5, 4, 3, 8]\n",
    "FNN = MultilayerPerceptronClassifier(featuresCol=\"features\" ,labelCol=\"indexedLabel\", maxIter=300, layers=layers)\n",
    "FNNModel = FNN.fit(trainingData).transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+------------+--------------------+--------------------+----------+\n",
      "|mono_ipc|         rawFeatures|            features|indexedLabel|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+--------------------+------------+--------------------+--------------------+----------+\n",
      "|       C|(17518,[0,2,3,4,6...|(17518,[0,2,3,4,6...|         0.0|[7.86917423544461...|[0.42011571546086...|       1.0|\n",
      "|       C|(17518,[0,2,4,19,...|(17518,[0,2,4,19,...|         0.0|[5.67270138747262...|[0.96143216333270...|       0.0|\n",
      "|       C|(17518,[0,4,14,18...|(17518,[0,4,14,18...|         0.0|[8.27611533230929...|[0.70972009652484...|       0.0|\n",
      "|       C|(17518,[5,6,10,16...|(17518,[5,6,10,16...|         0.0|[0.45946124204758...|[0.15760435821545...|       2.0|\n",
      "|       F|(17518,[1,17,71,8...|(17518,[1,17,71,8...|         5.0|[-3.6533299451317...|[7.18238339490464...|       3.0|\n",
      "|       G|(17518,[0,4,5,14,...|(17518,[0,4,5,14,...|         3.0|[-3.5251477305843...|[8.75784668361863...|       3.0|\n",
      "|       A|(17518,[0,2,9,11,...|(17518,[0,2,9,11,...|         1.0|[6.03738577604681...|[0.28489219986465...|       1.0|\n",
      "|       A|(17518,[0,4,9,25,...|(17518,[0,4,9,25,...|         1.0|[0.83268381400519...|[0.00203016946457...|       1.0|\n",
      "|       A|(17518,[0,30,37,3...|(17518,[0,30,37,3...|         1.0|[9.97735990936350...|[0.99886644607547...|       0.0|\n",
      "|       A|(17518,[2,5,10,14...|(17518,[2,5,10,14...|         1.0|[-3.6445365657021...|[7.28552436255411...|       3.0|\n",
      "|       B|(17518,[0,4,9,25,...|(17518,[0,4,9,25,...|         2.0|[0.83268381400519...|[0.00203016946457...|       1.0|\n",
      "|       C|(17518,[0,1,2,4,9...|(17518,[0,1,2,4,9...|         0.0|[-0.9922406553702...|[0.00781827002706...|       2.0|\n",
      "|       C|(17518,[0,1,4,13,...|(17518,[0,1,4,13,...|         0.0|[6.22694840954728...|[0.97529086182696...|       0.0|\n",
      "|       C|(17518,[0,1,4,19,...|(17518,[0,1,4,19,...|         0.0|[8.38656769651877...|[0.78181585849722...|       0.0|\n",
      "|       C|(17518,[0,4,14,18...|(17518,[0,4,14,18...|         0.0|[0.46027193643975...|[0.15772252035388...|       2.0|\n",
      "|       H|(17518,[0,4,12,13...|(17518,[0,4,12,13...|         4.0|[-1.4271363789419...|[0.00134749728855...|       4.0|\n",
      "|       H|(17518,[2,5,13,14...|(17518,[2,5,13,14...|         4.0|[-1.4271352156068...|[0.00134749865484...|       4.0|\n",
      "|       C|(17518,[0,1,4,5,1...|(17518,[0,1,4,5,1...|         0.0|[0.55546959259413...|[0.02708823429056...|       2.0|\n",
      "|       C|(17518,[0,2,3,4,5...|(17518,[0,2,3,4,5...|         0.0|[5.63025895765308...|[0.96009813674815...|       0.0|\n",
      "|       G|(17518,[0,14,30,4...|(17518,[0,14,30,4...|         3.0|[0.46022207010449...|[0.15771515103703...|       2.0|\n",
      "+--------+--------------------+--------------------+------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FNNModel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(probability=DenseVector([0.4201, 0.577, 0.0006, 0.0, 0.0, 0.0017, 0.0, 0.0006])),\n",
       " Row(probability=DenseVector([0.9614, 0.0034, 0.0132, 0.0003, 0.0014, 0.0177, 0.0, 0.0025])),\n",
       " Row(probability=DenseVector([0.7097, 0.287, 0.0008, 0.0, 0.0, 0.0019, 0.0, 0.0005])),\n",
       " Row(probability=DenseVector([0.1576, 0.0671, 0.2503, 0.219, 0.0869, 0.1011, 0.0186, 0.0994])),\n",
       " Row(probability=DenseVector([0.0007, 0.0124, 0.028, 0.7732, 0.0799, 0.0136, 0.0558, 0.0364])),\n",
       " Row(probability=DenseVector([0.0009, 0.0132, 0.0322, 0.7633, 0.0792, 0.0143, 0.0586, 0.0384])),\n",
       " Row(probability=DenseVector([0.2849, 0.7071, 0.0037, 0.0, 0.0, 0.0023, 0.0, 0.002])),\n",
       " Row(probability=DenseVector([0.002, 0.9881, 0.006, 0.0002, 0.0001, 0.0003, 0.0, 0.0034])),\n",
       " Row(probability=DenseVector([0.9989, 0.0001, 0.0007, 0.0, 0.0, 0.0004, 0.0, 0.0])),\n",
       " Row(probability=DenseVector([0.0007, 0.0124, 0.0286, 0.7724, 0.0791, 0.0135, 0.0567, 0.0365]))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_FNN=FNNModel.select('probability').collect();prob_FNN[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+--------------+\n",
      "|            features|mono_ipc|predictedLabel|\n",
      "+--------------------+--------+--------------+\n",
      "|(17518,[0,2,3,4,6...|       C|             A|\n",
      "|(17518,[0,2,4,19,...|       C|             C|\n",
      "|(17518,[0,4,14,18...|       C|             C|\n",
      "|(17518,[5,6,10,16...|       C|             B|\n",
      "|(17518,[1,17,71,8...|       F|             G|\n",
      "|(17518,[0,4,5,14,...|       G|             G|\n",
      "|(17518,[0,2,9,11,...|       A|             A|\n",
      "|(17518,[0,4,9,25,...|       A|             A|\n",
      "|(17518,[0,30,37,3...|       A|             C|\n",
      "|(17518,[2,5,10,14...|       A|             G|\n",
      "|(17518,[0,4,9,25,...|       B|             A|\n",
      "|(17518,[0,1,2,4,9...|       C|             B|\n",
      "|(17518,[0,1,4,13,...|       C|             C|\n",
      "|(17518,[0,1,4,19,...|       C|             C|\n",
      "|(17518,[0,4,14,18...|       C|             B|\n",
      "|(17518,[0,4,12,13...|       H|             H|\n",
      "|(17518,[2,5,13,14...|       H|             H|\n",
      "|(17518,[0,1,4,5,1...|       C|             B|\n",
      "|(17518,[0,2,3,4,5...|       C|             C|\n",
      "|(17518,[0,14,30,4...|       G|             B|\n",
      "+--------------------+--------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",labels=labelIndexer.labels)\n",
    "labelConverter.transform(FNNModel).select('features','mono_ipc','predictedLabel').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter.transform(FNNModel).select('indexedLabel','prediction').createOrReplaceTempView('table2')\n",
    "predictionAndTarget = spark.sql('select indexedLabel as label, prediction from table2')\n",
    "\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "acc = evaluatorMulti.evaluate(predictionAndTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39263883094815843\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
